{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractstaticmethod\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Dropout, Flatten, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import pickle\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "Utils\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPConfiguration(ABC):\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def build_hp_model(hp):\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_tuner():\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_callbacks():\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_tuner_callbacks():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_classes = [\n",
    "    'yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'\n",
    "]\n",
    "\n",
    "additional_classes = [\n",
    "    'zero', 'wow', 'two', 'tree', 'three', 'six', 'sheila', 'seven', 'one', 'nine', 'marvin', 'house', 'happy', 'four', 'five', 'eight', 'dog', 'cat', 'bird', 'bed'\n",
    "]\n",
    "\n",
    "all_classes = basic_classes + additional_classes\n",
    "\n",
    "class_name_to_idx = {class_name: idx for idx, class_name in enumerate(all_classes)}\n",
    "idx_to_class_name = {idx: class_name for idx, class_name in enumerate(all_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(type, full_class=True, augmentation='none'):\n",
    "    if type == 'mfcc':\n",
    "        if augmentation == 'none':\n",
    "            train_x = np.load(\"numpy_arrays/X_train_mfcc_unaugmented.npy\")\n",
    "            train_y = np.load(\"numpy_arrays/y_train_unaugmented.npy\")\n",
    "        elif augmentation == 'noice':\n",
    "            train_x = np.load(\n",
    "                \"numpy_arrays/X_train_mfcc_noice_augmentation.npy\")\n",
    "            train_y = np.load(\n",
    "                \"numpy_arrays/y_train_mfcc_noice_augmentation.npy\")\n",
    "        elif augmentation == 'pitch':\n",
    "            train_x = np.load(\n",
    "                \"numpy_arrays/X_train_mfcc_pitch_augmentation.npy\")\n",
    "            train_y = np.load(\n",
    "                \"numpy_arrays/y_train_mfcc_pitch_augmentation.npy\")\n",
    "        validation_x = np.load(\"numpy_arrays/X_validation_mfcc.npy\")\n",
    "        validation_y = np.load(\"numpy_arrays/y_validation_mfcc.npy\")\n",
    "        test_x = np.load(\"numpy_arrays/X_test_mfcc.npy\")\n",
    "        test_y = np.load(\"numpy_arrays/y_test_mfcc.npy\")\n",
    "    elif type == 'mel':\n",
    "        if augmentation == 'none':\n",
    "            train_x = np.load(\n",
    "                \"numpy_arrays/X_train_mel_spectogram_unaugmented.npy\")\n",
    "            train_y = np.load(\n",
    "                \"numpy_arrays/y_train_mel_spectogram_unaugmented.npy\")\n",
    "        elif augmentation == 'noice':\n",
    "            train_x = np.load(\n",
    "                \"numpy_arrays/X_train_mel_spectogram_noice_augmentation.npy\")\n",
    "            train_y = np.load(\n",
    "                \"numpy_arrays/y_train_mel_spectogram_noice_augmentation.npy\")\n",
    "        elif augmentation == 'pitch':\n",
    "            train_x = np.load(\n",
    "                \"numpy_arrays/X_train_mel_spectogram_pitch_augmentation.npy\")\n",
    "            train_y = np.load(\n",
    "                \"numpy_arrays/y_train_mel_spectogram_pitch_augmentation.npy\")\n",
    "        validation_x = np.load(\"numpy_arrays/X_validation_mel_spectogram.npy\")\n",
    "        validation_y = np.load(\"numpy_arrays/y_validation_mel_spectogram.npy\")\n",
    "        test_x = np.load(\"numpy_arrays/X_test_mel_spectogram.npy\")\n",
    "        test_y = np.load(\"numpy_arrays/y_test_mel_spectogram.npy\")\n",
    "\n",
    "    if full_class != True:\n",
    "        train_y[train_y > 9] = 10\n",
    "        validation_y[validation_y > 9] = 10\n",
    "        test_y[test_y > 9] = 10\n",
    "\n",
    "    return train_x, train_y, validation_x, validation_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawConfMatrix(model, x_test, y_test, labels):\n",
    "    predictions = model.predict(x_test, verbose=True)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    confMat = ConfusionMatrixDisplay(confusion_matrix(y_test, predictions, normalize='true'), display_labels=labels)\n",
    "    fig, ax = pyplot.subplots(figsize=(8,8))\n",
    "    confMat.plot(ax = ax,  xticks_rotation = 'vertical')\n",
    "\n",
    "def plot_accuracy(history, model_name):\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(history, model_name):\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_history(history, model_name):\n",
    "    plot_accuracy(history, model_name)\n",
    "    plot_loss(history, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_number = 11\n",
    "input_shape = (25, 32)\n",
    "#input_shape = (128, 32)\n",
    "\n",
    "class SimpleLSTMModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, lstm_units, dense_units, **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.lstm_1 = LSTM(\n",
    "            lstm_units, return_sequences=False, input_shape=input_shape)\n",
    "        self.dense_1 = Dense(dense_units, activation='relu')\n",
    "        self.dense_2 = Dense(class_number, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.lstm_1(inputs)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        lstm_units = hp.Int(\"lstm_units\", min_value=32,\n",
    "                            max_value=256, step=2, sampling=\"log\")\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=16,\n",
    "                             max_value=256, step=2, sampling=\"log\")\n",
    "        model = SimpleLSTMModel(\n",
    "            lstm_units=lstm_units, dense_units=dense_units\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SimpleLSTMModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            # overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SimpleLSTMModel',\n",
    "                            project_name='model'\n",
    "                            )\n",
    "\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SuperSimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "\n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "\n",
    "    def get_best_model():\n",
    "        tuner = SimpleLSTMModel.get_tuner()\n",
    "        best_parameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return SimpleLSTMModel.build_hp_model(best_parameters)\n",
    "\n",
    "\n",
    "class IntermediateLSTMModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, lstm_units_1, lstm_units_2, dense_units, **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.lstm_1 = LSTM(\n",
    "            lstm_units_1, return_sequences=True, input_shape=input_shape)\n",
    "        self.lstm_2 = LSTM(lstm_units_2, return_sequences=False)\n",
    "        self.dense_1 = Dense(dense_units, activation='relu')\n",
    "        self.dense_2 = Dense(class_number, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm_1(inputs)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=32,\n",
    "                              max_value=256, step=2, sampling=\"log\")\n",
    "        lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=32,\n",
    "                              max_value=256, step=2, sampling=\"log\")\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=16,\n",
    "                             max_value=256, step=2, sampling=\"log\")\n",
    "        model = IntermediateLSTMModel(\n",
    "            lstm_units_1=lstm_units_1, lstm_units_2=lstm_units_2, dense_units=dense_units\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(IntermediateLSTMModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            # overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/IntermediateLSTMModel',\n",
    "                            project_name='model'\n",
    "                            )\n",
    "\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SuperSimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "\n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "\n",
    "    def get_best_model():\n",
    "        tuner = IntermediateLSTMModel.get_tuner()\n",
    "        best_parameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return IntermediateLSTMModel.build_hp_model(best_parameters)\n",
    "\n",
    "\n",
    "class AdvancedLSTMModel(keras.Model, HPConfiguration):\n",
    "\n",
    "    def __init__(self, lstm_units_1, lstm_units_2, dense_units_1, dense_units_2, **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.lstm_1 = LSTM(\n",
    "            lstm_units_1, return_sequences=True, input_shape=input_shape)\n",
    "        self.lstm_2 = LSTM(lstm_units_2, return_sequences=False)\n",
    "        self.dense_1 = Dense(dense_units_1, activation='relu')\n",
    "        self.dense_2 = Dense(dense_units_2, activation='relu')\n",
    "        self.dense_3 = Dense(class_number, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.lstm_1(inputs)\n",
    "        x = self.lstm_2(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dense_3(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        lstm_units_1 = hp.Int(\"lstm_units_1\", min_value=32,\n",
    "                              max_value=256, step=2, sampling=\"log\")\n",
    "        lstm_units_2 = hp.Int(\"lstm_units_2\", min_value=32,\n",
    "                              max_value=256, step=2, sampling=\"log\")\n",
    "        dense_units_1 = hp.Int(\"dense_units_1\", min_value=16,\n",
    "                               max_value=256, step=2, sampling=\"log\")\n",
    "        dense_units_2 = hp.Int(\"dense_units_2\", min_value=16,\n",
    "                               max_value=256, step=2, sampling=\"log\")\n",
    "        model = AdvancedLSTMModel(\n",
    "            lstm_units_1=lstm_units_1, lstm_units_2=lstm_units_2, dense_units_1=dense_units_1, dense_units_2=dense_units_2\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(AdvancedLSTMModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            # overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/AdvancedLSTMModel',\n",
    "                            project_name='model'\n",
    "                            )\n",
    "\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SuperSimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "\n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "\n",
    "    def get_best_model():\n",
    "        tuner = AdvancedLSTMModel.get_tuner()\n",
    "        best_parameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return AdvancedLSTMModel.build_hp_model(best_parameters)\n",
    "    \n",
    "class SimpleGRUModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, gru_units, dense_units, **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.gru_1 = GRU(\n",
    "            gru_units, return_sequences=False, input_shape=input_shape)\n",
    "        self.dense_1 = Dense(dense_units, activation='relu')\n",
    "        self.dense_2 = Dense(class_number, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        x = self.gru_1(inputs)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        gru_units = hp.Int(\"gru_units\", min_value=32,\n",
    "                            max_value=256, step=2, sampling=\"log\")\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=16,\n",
    "                             max_value=256, step=2, sampling=\"log\")\n",
    "        model = SimpleGRUModel(\n",
    "            gru_units=gru_units, dense_units=dense_units\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SimpleGRUModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            # overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SimpleGRUModel',\n",
    "                            project_name='model'\n",
    "                            )\n",
    "\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SuperSimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "\n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "\n",
    "    def get_best_model():\n",
    "        tuner = SimpleGRUModel.get_tuner()\n",
    "        best_parameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return SimpleGRUModel.build_hp_model(best_parameters)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Hyperparameter tuning\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mfcc', False)\n",
    "\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_classes = [SimpleLSTMModel, IntermediateLSTMModel, AdvancedLSTMModel, SimpleGRUModel]\n",
    "model_classes = [SimpleGRUModel]\n",
    "# HYPERPARAMETER TUNING\n",
    "for model_class in model_classes:\n",
    "    print(model_class)\n",
    "    tuner = model_class.get_tuner()\n",
    "    #print(tuner)\n",
    "    tuner.search(x_train, y_train, batch_size=256, epochs=50, validation_data=(x_validation, y_validation), callbacks=model_class.get_tuner_callbacks())\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_class in model_classes:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(best_hps.values)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Class weight influence experiment\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "for model_class in [IntermediateLSTMModel]:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    for i in range(5):\n",
    "        keras.backend.clear_session()\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        history = best_model.fit(x_train, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=n_epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[early_stop],\n",
    "                                verbose=1)\n",
    "        best_model.save('models/' + model_class.__name__ + '/' + str(i))\n",
    "        with open(f'./train_history/' + model_class.__name__ + '_' + str(i), 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(list(enumerate(class_weights)))\n",
    "\n",
    "for model_class in [IntermediateLSTMModel]:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    for i in range(5):\n",
    "        keras.backend.clear_session()\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        history = best_model.fit(x_train, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=n_epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[early_stop],\n",
    "                                verbose=1,\n",
    "                                class_weight=class_weights)\n",
    "        best_model.save('models/' + model_class.__name__ + '_cw/' + str(i))\n",
    "        with open(f'./train_history/' + model_class.__name__ + '_cw_' + str(i), 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Different networks on MFCC\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "for model_class in [SimpleGRUModel, SimpleLSTMModel, AdvancedLSTMModel]:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    for i in range(5):\n",
    "        keras.backend.clear_session()\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        history = best_model.fit(x_train, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=n_epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[early_stop],\n",
    "                                verbose=1)\n",
    "        best_model.save('models/' + model_class.__name__ + '/' + str(i))\n",
    "        with open(f'./train_history/' + model_class.__name__ + '_' + str(i), 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Augmentations\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mfcc', False, augmentation='noice')\n",
    "\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "for model_class in [IntermediateLSTMModel]:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    for i in range(5):\n",
    "        keras.backend.clear_session()\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        history = best_model.fit(x_train, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=n_epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[early_stop],\n",
    "                                verbose=1)\n",
    "        best_model.save('models/' + model_class.__name__ + '_noice/' + str(i))\n",
    "        with open(f'./train_history/' + model_class.__name__ + '_noice_' + str(i), 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mfcc', False, augmentation='pitch')\n",
    "\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "for model_class in [IntermediateLSTMModel]:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    for i in range(5):\n",
    "        keras.backend.clear_session()\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        history = best_model.fit(x_train, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=n_epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[early_stop],\n",
    "                                verbose=1)\n",
    "        best_model.save('models/' + model_class.__name__ + '_pitch/' + str(i))\n",
    "        with open(f'./train_history/' + model_class.__name__ + '_pitch_' + str(i), 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Dropout & L2 Regularization\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.layers import Dropout\n",
    "\n",
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mfcc', False)\n",
    "\n",
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dropout_model(dropout, input_shape=(25, 32)):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(LSTM(256, return_sequences=False))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(class_number, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_regularization_model(reg, dropout=0.0, input_shape=(25, 32)):\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape, kernel_regularizer=reg))\n",
    "    if dropout > 0.0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(LSTM(256, return_sequences=False, kernel_regularizer=reg))\n",
    "    if dropout > 0.0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=reg))\n",
    "    if dropout > 0.0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(class_number, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    keras.backend.clear_session()\n",
    "    model =  create_dropout_model(0.2)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=(x_validation, y_validation),\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1)\n",
    "    model.save('models/d02/' + str(i))\n",
    "    with open('./train_history/d02_' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "for i in range(5):\n",
    "    keras.backend.clear_session()\n",
    "    model =  create_dropout_model(0.1)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=(x_validation, y_validation),\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1)\n",
    "    model.save('models/d01/' + str(i))\n",
    "    with open('./train_history/d01_' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    keras.backend.clear_session()\n",
    "    model =  create_regularization_model(l2(0.02))\n",
    "    history = model.fit(x_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=(x_validation, y_validation),\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1)\n",
    "    model.save('models/r002/' + str(i))\n",
    "    with open('./train_history/r002_' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "for i in range(5):\n",
    "    keras.backend.clear_session()\n",
    "    model =  create_regularization_model(l2(0.01))\n",
    "    history = model.fit(x_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=(x_validation, y_validation),\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1)\n",
    "    model.save('models/r001/' + str(i))\n",
    "    with open('./train_history/r001_' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    keras.backend.clear_session()\n",
    "    model =  create_regularization_model(l2(0.02), 0.2)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=(x_validation, y_validation),\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1)\n",
    "    model.save('models/r002d02/' + str(i))\n",
    "    with open('./train_history/r002d02_' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "for i in range(5):\n",
    "    keras.backend.clear_session()\n",
    "    model =  create_regularization_model(l2(0.01),0.2)\n",
    "    history = model.fit(x_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=(x_validation, y_validation),\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1)\n",
    "    model.save('models/r001d02/' + str(i))\n",
    "    with open('./train_history/r001d02_' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "CNN\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mel', False)\n",
    "\n",
    "input_shape = (128, 32, 1)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "n_classes = 11\n",
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=5,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "def create_vgg_model(con_base, n_classes, optimizer, fine_tune=False):\n",
    "\n",
    "    conv_base = con_base\n",
    "    \n",
    "    for layer in conv_base.layers:\n",
    "        layer.trainable = fine_tune\n",
    "\n",
    "    top_model = conv_base.output\n",
    "    top_model = Flatten()(top_model)\n",
    "    top_model = Dense(4096, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "    top_model = Dense(4096, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.5)(top_model)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    print(i)\n",
    "    keras.backend.clear_session()\n",
    "    vgg16 = tf.keras.applications.VGG16(include_top=False,\n",
    "                                    weights=None,\n",
    "                                    input_shape=input_shape)\n",
    "    vgg_model = create_vgg_model(vgg16, n_classes, optimizer, fine_tune=True)\n",
    "    vgg_history = vgg_model.fit(x_train, y_train,\n",
    "                            batch_size=32,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=(x_validation, y_validation),\n",
    "                            callbacks=[early_stop],\n",
    "                            verbose=1)\n",
    "    vgg_model.save('models/vgg16_/' + str(i))\n",
    "    with open('./train_history/vgg16_' + str(i), 'wb') as file_pi:\n",
    "        pickle.dump(vgg_history.history, file_pi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Different networks on MEL\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mel', False)\n",
    "\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "#for model_class in [IntermediateLSTMModel, AdvancedLSTMModel]:\n",
    "for model_class in [AdvancedLSTMModel]:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    for i in range(2,5):\n",
    "        keras.backend.clear_session()\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        history = best_model.fit(x_train, y_train,\n",
    "                                batch_size=32,\n",
    "                                epochs=n_epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[early_stop],\n",
    "                                verbose=1)\n",
    "        best_model.save('models/mel_' + model_class.__name__ + '/' + str(i))\n",
    "        with open(f'./train_history/mel_' + model_class.__name__ + '_' + str(i), 'wb') as file_pi:\n",
    "            pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "Models reader & statistics generator\n",
    "</h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "Model reader\n",
    "</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mfcc', False)\n",
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_datasets('mel', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'models/mel_AdvancedLSTMModel/'\n",
    "accuracies = []\n",
    "\n",
    "for i in range(5):\n",
    "    model = keras.models.load_model(folder_path + str(i))\n",
    "    test_loss, test_accuracy = model.evaluate(x=x_test, y=y_test) \n",
    "    accuracies.append(test_accuracy)\n",
    "\n",
    "avg = np.average(accuracies)\n",
    "std = np.std(accuracies)\n",
    "minimal = np.min(accuracies)\n",
    "maximal = np.max(accuracies)\n",
    "print(f\"model: {folder_path}, avg: {avg}, std: {std}, min: {minimal}, max: {maximal}\")\n",
    "print('==============')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "History reader\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './train_history/mel_AdvancedLSTMModel_2'\n",
    "\n",
    "with open(filename, \"rb\") as file_pi:\n",
    "    history = pickle.load(file_pi)\n",
    "\n",
    "plot_history(history, 'Advanced LSTM Model on Mel dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
